[ { "title": "WiCyS 2025 Workshop - Phishing Resistance of Passkeys", "url": "/posts/WICYS-workshop/", "categories": "Presentations", "tags": "WiCyS", "date": "2025-04-05 04:00:00 +0000", "snippet": "IntroThis year at WiCyS 2025, held in Dallas, Texas, Nat and I had the pleasure of running a workshop on the last day of the conference. Despite it was the final session, the room was packed, the e...", "content": "IntroThis year at WiCyS 2025, held in Dallas, Texas, Nat and I had the pleasure of running a workshop on the last day of the conference. Despite it was the final session, the room was packed, the energy was high, and the engagement was fantastic. We’re publishing this article as a follow-up to the session—both to answer some of the recurring questions and to share the insights with those who couldn’t make it.Our objective was twofold: Show, not tell, why some Multi-Factor Authentication (MFA) methods fail against phishing attacks. Introduce passkeys and give participants a first-hand experience of how they address those very issues.Workshop WalkthroughWe had a full two hours together, and we made the most of it.We began by discussing the well-known flaws of passwords and gradually moved to the out-of-band MFA methods: SMS-based one-time passwords (OTPs), email OTPs, push notifications.Then came Lab 1: the phishing simulation.We invited participants to take the place of a victim in a controlled, hands-on phishing exercise. Using a real-time demo with Evilginx, everyone witnessed how attackers can intercept credentials and session tokens - enough to impersonate users even when MFA is enabled.The participants could: Receive a simulated phishing email Log in “as normal” Watch how their credentials and session cookies were captured See how the attacker could immediately reuse the session without knowing the password or OTPIt was a little unsettling - but that was the point.Lab 2: Fighting back with passkeys.In the second half, we introduced passkeys. We covered their: Design principles Usability improvements Most importantly, their phishing resistanceEveryone had a chance to register a passkey and then test whether our earlier phishing technique would still work.Participants left the room with a deeper understanding of what phishing-resistant authentication looks like—and why it’s the future.SetupThe tech stack for the workshop was intentionally simple to help others replicate or explore it themselves:AitM backend: Evilginx running on a virtual machinePhishing lure email: A mock single-page email client, built to deliver a convincing fake GitHub emailSession viewer: Our custom-built web viewer for Evilginx session dataPhishing domain: github.cybersoup.orgTarget platform: Actual GitHub accounts (github.com)Evilginx tutorial is available here.Workshop presentation is here.Final WordsWiCyS conference continues to be an amazing event — where students, researchers, career changers, and cybersecurity oddballs like us come together to share and grow.We’re proud allies and thrilled to have been a part of this year’s edition.There’s something uniquely Texan about line dancing to country music and watching longhorns roam the Fort Worth Stockyards — an unexpected but memorable part of the WiCyS experience.Thanks to everyone who joined us. Until next time!" }, { "title": "Cybersecurity - the longest journey", "url": "/posts/GIAC-notes/", "categories": "Lessons learned", "tags": "GIAC, SANS, WiCyS", "date": "2024-12-19 04:00:00 +0000", "snippet": "Part One: The CallingEver since I joined the ranks of IT professionals, I’ve been constantly surprised by how patchy cybersecurity awareness is in some organizations. This unfortunately translates ...", "content": "Part One: The CallingEver since I joined the ranks of IT professionals, I’ve been constantly surprised by how patchy cybersecurity awareness is in some organizations. This unfortunately translates into uneven and often inadequate security controls. As much as one might be reluctant to admit it, securing organizational assets is an uphill battle—especially when there’s never enough budget to implement the right solutions in the right way. To make matters worse, when cybersecurity is treated as a liability, it often proves to be exactly that. A self-fulfilling crisis, if you will.Let’s run a quick test, shall we?How many of you have seen login credentials for both test and production accounts stored in plaintext on an internally available website? How about the all-time classic—API secrets accidentally pushed to a repository (extra points if the repo was publicly available)? Yes, you can rotate the secrets and clear out the Git history so no trace of the mishap remains, but what lesson have you learned? And what can be done to ensure it doesn’t happen again to anyone else, including your future self?I was introduced to good cybersecurity practices during my first gig as a budding quality analyst back in Poland (here’s to you, dear Sagiton folk). My team taught me the value of static code analysis and automated security checks even before I could fully grasp their significance in strengthening the security posture of a system. Many years later, I found myself regularly revisiting those areas that had always sparked my interest, but which I’d never really had the chance to fully explore. It was clearly something I was drawn toward, no matter my role in the past. Soon, I realized it was only a matter of time before I made the pivot. The only thing I didn’t know was when and how the pivot would happen. I admit I dragged my feet for months, waiting for a sign—even if I didn’t necessarily believe in the supernatural.This is why, when I first heard of the WiCyS scholarship, I thought: Well, this is it. You’re not getting a better chance. Go for it. See what happens. So, that’s exactly what I did—starting with the initial Capture the Flag event back in November 2023, then somehow making it into Tiers Two and Three, and finally becoming one of the sixty-odd women who advanced to the final Tier.Was it one of the most intense and challenging years of my life? Heck yes!Knowing what I know now, would I do it all over again?In a heartbeat.Part Two: The CompanyYou see, there are at least two choices when you find your calling and decide to act on it: you either try to go it alone, or you gather your party before venturing forth. I’m not a big fan of survival mode, so I wholeheartedly welcomed the opportunity to gain new skills and knowledge alongside some of the most talented and determined women I’ve ever had the chance to meet. Months of Tier 1-3 challenges had already brought us closer together, and even if we couldn’t compare all our notes (intellectual property struggles are real!), the sense of a unified community was definitely there. Weekly online catch-ups only strengthened the bond, with WiCyS alumni supervising our progress and offering help whenever it was needed.And the best part? All program members were granted a scholarship to attend the annual WiCyS conference, which this year took place in Nashville, Tennessee. I wish I could offer a longer description of how electrifying the conference was or how everything about it felt like finally coming home after years of aimless wandering—but I can’t and won’t. The three days flew by so quickly that I could hardly catch my breath, let alone find everyone I’d made a solid plan to grab coffee with.This is why I promised myself to go back as soon as possible—and plan it better this time, allowing enough time for both learning and making new friends. After all, both are equally important.Part Three: The QuestThe main part of the scholarship included access to three cybersecurity courses developed by the SANS Institute, a world-renown resource for high-quality cybersecurity education. Each course concluded with two GIAC practice exams, and passing at least one of them with a satisfactory score (80% in our case) unlocked an attempt at the real exam. The linear structure of this main quest worked wonders for a busy brain like mine, making it relatively easy to focus on the three-part journey ahead.Here are some lessons learned from each stage:GIAC Foundational Cybersecurity Technologies (GFACT) The course was a great introduction to IT, computing and security essentials, presented in an engaging way by James Lyne, the CTO of SANS. What stood out the most was James’ passion for every topic he covered, from hardware components to cyber forensics, along with his witty analogies that made seemingly unrelated concepts both entertaining and easier to grasp (e.g., why computer networks are like Rolo chocolates). I had a few years of IT experience under my belt, so achieving a 96% score on my final exam was relatively easy. Was it completely effortless? Absolutely not! Since I’d never earned a computer science or IT degree, there were certain gaps in my understanding that the course helped me fill. Some gaps remained—hence the missing 4%—but that was an error margin I could definitely live with. The main challenge of this stage was learning how to navigate the scholarship program and stay consistent with weekly Zoom check-ins. Due to time zone differences, these sessions fluctuated between 6–7 a.m., which gave me a newfound appreciation for earlier bedtimes. Once I found my rhythm, though, nothing could distract me from my goal of sitting the exam and passing it within the first two months of starting the course.GIAC Security Essentials (GSEC) If GFACT was a stepping stone, I would compare GSEC to a rabbit hole of the most intriguing kind—the depth and breadth of the course material were not for the faint-hearted. If that wasn’t enough, the course included multiple resources to explore in your own time. With the staggering number of areas covered, the author, Bryan Simon, did a fantastic job maintaining an even pace and high level of engagement throughout. I especially appreciated his anecdotes, which were a welcome break when the material became a bit too technical to stay focused for extended periods. What I enjoyed most at this stage was the hands-on aspect of the course. As someone with an extremely practice-oriented brain, I need to get my hands dirty with something new before I can confidently say I understand its inner workings. The lab setup and tasks provided a fresh, practical level of insight that I’d been looking for, helping to sharpen my PowerShell and Bash skills well beyond the everyday basics. I passed my final exam with a score of 89% in three months, which I was quite satisfied with. I’m fairly certain I performed better on the lab section than the practical one, due to a small mishap—three-quarters into the exam, I accidentally clicked the button to answer all the skipped questions, forcing me to speed-run through them and the remainder before I could even get to the hands-on portion. Despite the incident, my strategy to divide the four-hour exam into two halves almost worked, even if I felt like an internally screaming possum during the second half.GIAC Certified Incident Handler Certification (GCIH) The last exam was the biggest challenge—not because of the material (although its depth was truly impressive, and I thought nothing could surprise me after GSEC!) but because of my time blindness. Despite a planned trip to the U.S. to attend the annual WiCyS conference, followed by a 1.5-month family visit to Poland, and simultaneously juggling an Advanced Diploma of IT course, I decided to start my GCIH prep before the flight to the U.S. Mistake number one! Unsurprisingly, I couldn’t maintain my focus on the course material during the first month. Even though I had all my printed course books with me, traveling with an extra five kilos of paper wasn’t always the most practical choice (mistake number two). Mistake number three was assuming—based on my luck with lab questions on the GSEC exam—that practicing the course labs only once and simply rereading them for revision would suffice. It didn’t, and my final score of 87% reflected my poor judgment. Yet another lesson learned. :) That said, I thoroughly enjoyed the prep and exam experience itself. Hats off to the course author, Joshua Wright, who managed to make it as engaging and entertaining as a conversation with a good friend, never losing momentum. Even if I never work as an incident handler, the transferable skills and knowledge I gained are already proving useful in my current areas of focus.Part Four: The FutureHaving completed both the security training scholarship and my Advanced Diploma of IT (Cybersecurity), I can confidently say I see myself thriving in this field over the next few years. I’ve already been fortunate enough to secure a position as an Identity Engineer, joining not one but two fantastic teams filled with kind and talented professionals.As I continue this exciting journey, I’ll share more lessons learned and create guides in the hope that, one day, they might help solve your problems and make your lives easier.Identity and access management is a cornerstone of every system you can imagine, so you’d better pack a good pair of walking boots.Next stop: everywhere!" }, { "title": "Keeping secrets secret", "url": "/posts/secret-zero/", "categories": "Secrets, IAM", "tags": "AppSec, Intermediate", "date": "2024-12-19 04:00:00 +0000", "snippet": "IntroductionSoftware development is often seen as a blend of engineering and art — a creative process where developers work withinthe boundaries of project requirements, timelines, technological li...", "content": "IntroductionSoftware development is often seen as a blend of engineering and art — a creative process where developers work withinthe boundaries of project requirements, timelines, technological limitations, and team capacity.The goal? To produce human-readable code that, through various transformations, solves a specific problem.From a business standpoint, software is primarily judged by its functionality and reliability.Metrics like maintainability, cleanliness, and security frequently take a back seat.However, neglecting these aspects, particularly security, accumulates technical debt—a silent burden that oftenonly becomes apparent when it’s too late.In the realm of security, this issue is particularly pressing. Security debt doesn’t give immediate warning signs.It’s a ticking time bomb that may go unnoticed until a breach or vulnerability makes its presence painfully clear.So, what is the industry’s answer to this? “Shift left.” It’s a buzzword you’ve probably heard at conferencesand in thought leadership pieces. The idea is straightforward: address security issues as early as possible in theSoftware Development Lifecycle. But in practice, integrating security into the early phases of software design is easier said than done.This article focuses on a specific aspect of designing security into software — handling secrets.Secrets are everywhere in modern applications, whether they’re used to connect to databases, access third-party APIs,or authenticate with cloud services. Unfortunately, handling secrets is left for developers to figure out on their ownwithout proper guidance, tools, or understanding of the security implications.Here’s how the problem typically unfolds: a developer, working under time pressure, integrates an SDK to connect a service.They find an example snippet of connection code (or generate it with AI) and paste it into the codebase.These examples are designed for simplicity and clarity, often embedding connection credentials directly in the code.Without second thoughts, this practice makes its way into production.Is this the developer’s fault? Not entirely. Developers are not security experts, and they rely on what’s provided — documentation,examples, and tooling. If the sample code suggests embedding secrets directly, they follow without second thoughts.Through countless source code reviews, I’ve noticed that secrets management is rarely top-of-mind for developers juggling competing priorities.As a developer myself, I empathize with these challenges. Security often feels like an external pressure,a “nice-to-have” rather than a “must-have.” But this mindset needs to change.In this article, I aim to bring a software-focused view on the secret management solutions. We’ll explore practical strategies for securely handling secrets, evaluate various methods and discuss some challenges that may arise.(Note: The perspectives shared in this article are my personal views and should be interpreted as such.)Secrets handlingHandling secrets securely is a complex challenge with no one-size-fits-all solution. The approach varies depending onthe deployment environment — secrets for an application running on a bare-metal server are managed differently than thosefor a serverless application hosted by a cloud provider. Even among cloud vendors, secret management practices can differ significantly.Additionally, the type of secret matters; for instance, some secrets require frequent rotation to maintain security,while others are designed to be long-lived.Fortunately, there are common patterns that can be applied across many scenarios. In the followingsections, I will outline the different types of secrets and provide practical methods for managing them effectively.Types of secretsIdentifier and passwordWhat is the easiest way to protect a valuable resource? Introduce a password.But it is not good idea for everyone to use the same password. So let’s add an identifier to the password, so we can distinguish between users.An identifier and password pair is probably the oldest and most common mechanism.You can find it in almost every system that requires authentication, and even if stronger methods are present, this one is still used for fallback.It’s simple, easy to implement and understand, and widely supported. Usable for both human and machine authentication.The identifier (e.g., username, email, account number) is typically a publicly known value that is used to locate user entry.Then the secret part, e.g., password, known only to server and client and therefore can be used to authenticate.Examples: username and password (any user-based authentication) client_id and client_secret (OAuth2) role_id and secret_id (Hashicorp Vault AppRole)TokensWhat if the communication is server-to-server? Then, identifier and secret can be in one value - token.The identifier and secret can be combined into a single value—a token. Tokens come in various forms: they can carry information (e.g., JWT), be time-limited (e.g., TOTP), or be opaque (e.g., API key). Typically, the tokens requiring secure handling are bearer tokens that are non-expiring and used for API access.Bearer tokens mean the token alone is sufficient to authenticate; no additional information is needed. This also means that anyone, including an attacker, can use the token if they obtain it. Non-expiring tokens remain valid until explicitly revoked. If such a token is leaked, an attacker can continue using it until it is revoked.Developers typically create an account with a third-party service, generate a token, and use this token in their application. The token is then attached to each request made by the application.Examples: API key AIzaSyDaGmWKa4JsXZ-HjGw7ISLn_3namBGewQe JWT token eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJDeWJlclNvdXAiLCJpYXQiOjE3MzM2MjczMTQsImV4cCI6MTc2NTE2MzMxNCwiYXVkIjoid3d3LnJlYWRlci5jb20iLCJzdWIiOiJhd2Vzb21lQGV4YW1wbGUuY29tIiwiR2l2ZW5OYW1lIjoiQXdlc29tZSIsIlN1cm5hbWUiOiJHdXkiLCJFbWFpbCI6ImF3c29tZUBleGFtcGxlLmNvbSJ9.tCdcOT4NBelKf7Tmi88COsFh5Tf6_KMlJAg_danXs8E Cryptographic keysSecrets aren’t just for authentication—they’re also critical for cryptographic operations. Whenever an application needsto encrypt, decrypt, or sign data, it relies on a key. Whether it’s using symmetric or asymmetric cryptography,and regardless of the algorithm, these keys are considered secrets and must be carefully protected within the application.From a developer’s perspective, implementing cryptographic operations can be a headache. Different programming languagesand libraries provide inconsistent interfaces, and documentation is often unclear. To make matters worse, the way keysare formatted and represented varies widely. For example, some libraries expect keys in base64 format, while othersrequire them to be stored in a keystore or loaded from the file system. This inconsistency leads to confusion, mistakes,and often delays in implementation.Examples:-----BEGIN RSA PRIVATE KEY-----MIICXAIBAAKBgQDLKHCTCfANm8BbfrEG1U5jknlPqK/HzT2vK5uzzHuBQFfQRUObzuv39apu15AjA4FOYtWQtDEGabjGgkgMvneRS8PVD+pOYr/KdW/g8Ldjf4DlGPAXNXuuSm6J0rWcx298AQEE7MRWHzZmx4+HD0Kl7DtpLpZQbkd26NbDzxHqVQIDAQABAoGAQ0SMoeM0fQ0JUEJO03hlkEq7qEiui3XF6u6Bi7M1YcKwUOHeVQTa1Sue1zzBGRbcBdxr4pIHeZwf9nrE6JNYe0hTSZA8FzUkK+tzjHf0dyIvNrSRZSbUUgixOY9y4PbndVQB2mvZf/c3uXV0F1UPVx8yUYPvXbrfAGtdd3eKFYECQQDyJG1eBSqtV9s4Hn2ajEfqh8n+fsM3/dVwy/8ucAt8bX9+ki0qOSVcHUopLrzubuCqFZAheUYoaZvASgBqNFsJAkEA1sjcon3xoGoLptaynq/QVNzgWwsMo0vRDVDIo9eEoBWdPchg9xz1lCYUrVGV4GH+8MbxR54I+PClSbOQ+/5L7QJAaGjiq87ohxRCF6G2IUTp4awpok5AmU0fkuKzpu9zVHTWq9oWFYXMoTqT9swLdXhj8ZMYsgZcBSL8oN6H7UOkGQJAAnHi4Q6h83uBinKjMd86ddzVyPOFP06boJUs04Ceh9v3ID7pq6ZuvTL5xUdKd3VmG4OIN5J686p5ly8uFVA9uQJBAMbn43f943l5zJIrCVFR13+eSBKpWz1sYiKHQZSF/ZuSOqoKKoknYBUjYRF/rusSadh/e7PMn4Dcc1W32SviYwU=-----END RSA PRIVATE KEY-----Rotation &amp; dynamic secretsSome secrets are designed to last a long time—they remain valid until explicitly revoked. In some systems, this mightbe fine if the risk is low. But for others, it’s a legacy approach that fails to account for modern security threats.From a developer’s perspective, static secrets are incredibly convenient. You configure them, deploy your application,and forget about it. From a security perspective, though, this approach is a disaster. If a static secret gets leaked—say,it’s accidentally committed to source code—it’s game over. Anyone who gets their hands on it can use it. Worse,the resource server can’t distinguish between a legitimate request from the secret’s owner and a malicious request from someone who stole it.One way to reduce this risk is by rotating secrets. This involves periodically updating the secret on both the clientand server sides. It’s a great idea in theory, but in practice, it raises some tough questions: How do we handle secretrotation in our application?“IDK, we just have one static file with all the secrets saved on disk,” says the developer.Great, so implementing rotation isn’t going to be simple. And then the plot thickens:“We need to implement dynamic secrets for database access“ announces the manager, reading from a security audit report.“Dynamic secrets? What are those?” asks the developer.And thus begins the next chapter of the developer’s security saga. Dynamic secrets are generated on the fly—createdonly when an application or user needs access to a resource. Unlike static secrets, they’re valid only for a short periodof time and don’t need to be stored long-term.Sounds promising, right? But wait… How are these secrets generated? Who’s responsible for the process? And how do we verify the requester?We will see that later.Example: Hashicorp Vault dynamic secrets AKeyless dynamic secretsSolutionsLet’s explore the different ways to manage secrets in an application. We’ll start with the simplest and most obvioussolutions (the ones you definitely shouldn’t use) and gradually move toward more advanced and secure approaches.As you’ll notice, the level of effort required to implement a solution often goes hand in hand with the level of securityit provides. That’s why it’s always about finding the right balance between how secure your application needs to be andhow much effort you’re willing to invest in achieving it.Hardcoded secretsA quick side project is nearly done, running smoothly on a local machine. It connects to a cloud database using a secret.Since it’s just a simple, one-file proof of concept, the developer hardcodes the secret directly into the code.No big deal, right? But then the team decides to use it for an internal project, so the developer pushes the code to arepository. And just like that, the secret is in the repo.This scenario is as old as software development itself. The moment a file leaves the developer’s machine, any secrets init should be considered compromised. Whether it’s pushed to a Git repository, shared via email, sent on Slack, or uploadedto cloud storage, the secret is now effectively in the wild.How often does this happen? All the time. According to the GitGuardian report, over 12 million secrets were foundin public Git repositories in 2023 alone.Every Application Security guideline tells you: don’t store secrets in your code. Yet it still happens even to the bestof us, simply because it’s quick, easy, and convenient.So what should I do instead? - asks the developer.We will see better options in the next sections, for now let’s look what can be done to prevent hardcoded secrets.My advice? Automation, automation, and more automation. The earlier we can detect secrets, the better.Tools like pre-commit Git hooks, such as git-secrets, can catch issues before the code even makes it to the repository.However, since developers can bypass these checks, it’s essential to have similar scanning in place in the CI pipeline (e.g., TruffleHog).As much as I love automation, I’ll admit it’s not perfect—it won’t catch everything. That’s why manual code reviews arestill important. And let’s be honest, developers are human. If they’re not given clear instructions, they’ll often takethe easiest path. So, instead of just expecting them to get it right, we need to make sure they’re trained properly first.Examples:(Please don't do it)from Crypto.Cipher import AESobj = AES.new('secret', AES.MODE_CFB, 'ivvalue')message = \"Seriously, don't do it.\"ciphertext = obj.encrypt(message)Evaluation     Effort Almost none Security None Recommendation Big no no Environment variablesSo, what’s the first step to keep secrets without hardcoding them? The typical approach is to put a placeholder in the code and resolve the actual secret later. But where do we get the secret from?The obvious first choice is environment variables.These are key-value pairs available when the application runs. They come from the execution environment, whether it’s the operating system, a container, or a cloud provider. This way, secrets don’t need to be packaged with the code but can instead be referenced in the code and resolved at runtime.Sounds like a great solution, right? It’s definitely better than hardcoding secrets. It shifts the responsibility from the developer to someone else—like the system administrator. But let’s dig into where the secret actually lives now.In the case of a simple VM, the environment variable needs to be set in the VM OS for the application to access it. This can happen when the application starts (e.g., in a bash script) or in the user configuration running the app.In containerized environments, like Kubernetes, environment variables are often set in the deployment configuration. And in serverless environments, like AWS Lambda, you can set environment variables directly in the function configuration.If access to the environment is well protected with MFA, and everything’s logged and monitored, then maybe this is secure enough for your risk appetite. But let’s be real — manual configuration comes with its own set of headaches.Copying configurations, injecting secrets from a password manager, and switching values between environments, among others, can easily result in late-night, stressful, and hours-long deployment processesSo, let’s bring in some DevOps trends like GitOps and Infrastructure as Code (IaC). This means your infrastructure is defined as code, and guess where that code is stored? In a repository.So, does this mean our precious secrets are back in the repository? Have we just swapped one repo for another?Well, if you stop thinking right here, then yep, that’s exactly what’s happened!Examplesconst user = process.env.USERNAME;const password = process.env.PASSWORD;const apiKey = process.env.API_KEY;Evaluation     Effort Minimal Security Better than hardcoded passwords, but still poor Recommendation If you can’t do any better FilesNot too far from environment variables are files—config files, properties files, JSON files, YAML files, and so on. These are often used to store application settings, and sometimes, the secrets sneak in there too. Usually, these files are kept next to the application with at least read permissions. When the application starts, it reads the file and loads the secrets into memory.Sometimes, config files don’t directly hold secrets but rather point to environment variables where the secrets live. This means the secret is resolved at runtime, and the config file doesn’t need to actually store the sensitive data. This gives developers flexibility to locally work with typically single configuration file without setting up the environment,whereas the environment version of the file can use environment variables.So, is this really much different from environment variables? Not really. It’s just another way to store the secret within the environment.But let’s again add GitOps into the mix, and things might get a little worse. For instance, if your secrets are tucked into config files that are versioned in Git, you might just find yourself back at square one with the problem of hardcoded secrets again.ExamplesserviceUrl: https://example.comusername: userpassword: secretserviceUrl: https://example.comusername: userpassword: ${SECRET_PASSWORD}Evaluation     Effort Minimal Security Better than hardcoded passwords, but still poor Recommendation If you can’t do any better VaultsWhat if you could offload the responsibility of storing and controlling access to secrets to a specialized service? Well, that’s exactly what vaults are for.Vaults are centralized services designed to securely store secrets—encrypted, of course—and provide fancy features like advanced access control and auditing. Instead of having secrets sitting right next to your application, now your app has to go ask the vault for them.Just like with environment variables, the application uses placeholders for the secrets and resolves them at runtime, but now it’s all handled by the vault.The best part? Vaults give you total control. You get a clear overview of which secrets exist, who can access them, and when they’ve been accessed. You can set up complex policies to make sure only certain applications get to touch certain secrets. Plus, you can decide how long an app can have access to a secret before you revoke it.Examples Hashicorp Vault AWS Secrets Manager Azure Key Vault GCP Cloud Key ManagementEvaluation     Effort Medium Security Ain’t too bad Recommendation It is the minimum you should do Secret Zero ProblemThere is one question that is not answered yet.We can’t give everyone access to the vault, so how does an application authenticate to it?Good question! Unfortunately, the straightforward answer might not be all that satisfying.The simplest approach is to use credentials to authenticate to the vault. But wait—where does the application store those credentials? And just like that, we’ve stumbled into a chicken-egg problem: to access secrets, you first need a secret.There’s even a name for this tricky situation: the Secret Zero Problem.“What’s the point of having a vault if you still need a secret to access it?” asks the Developer.A fair question, and one that highlights the Secret Zero Problem. No matter how you slice it, there’s always a secret somewhere.But here’s the key: while we can’t eliminate the problem entirely, we can manage it better by putting stricter controls on the secret used to access the vault.For example, in a Kubernetes setup, the secret to access the vault might only be injected into pods that are signed and verified by our CI pipeline. This ensures the secret is tightly scoped and only accessible where it’s truly needed.What about the vault itself? At the end of the day, it’s just another app, usually with an admin or root account that has all the power. But vaults are built with strong security features to keep things under control. For example, many vaults come with sealing by default. When the vault restarts, it stays locked until someone unseals itwith a special secret. Using just one secret would be risky, but access can be configured to split the secret into pieces, and then shared among several people. That way, no one person can unlock the vault by themselves, making it much harder for anyone to break in.So yes, secrets are unavoidable. But by using dedicated tools like a vault, we gain the ability to apply robust controls, reduce exposure, and make the problem far more manageable.Cloud IdentityLet’s take a closer look at how an application can securely access other services, like vaults. Traditionally, this trust is built by having the service generate a secret and then handing it over to the application. But what if we rethink this model and introduce a third party to handle the trust?We already see similar approaches working well for users with technologies like OAuth2, SAML, and OpenID Connect. Interestingly, cloud providers have extended this concept to applications too.Here’s how it works: when an application is deployed in a cloud environment, the cloud provider can assign it an identity. Services like vaults then trust the cloud provider and grant access to the application based on this identity.The proces eliminates the need for the application to handle static secrets directly.But how does this actually work for a developer?In practice, the application requests an identity JWT token from a dedicated cloud endpoint available only to running applications. This means there’s no need to store or configure secrets in environment variables or files. It’s a clean, secure solution that simplifies secret management.Once authenticated using the cloud-provided identity, the application can access the secrets it needs from the vault.“With cloud identity, we finally don’t have any secrets” says developer.Well, to be precise, we haven’t eliminated the Secret Zero Problem entirely. Instead, we’ve shifted the responsibilityto the cloud provider—a tradeoff that often aligns well with most organizations’ risk tolerance.Evaluation     Effort Medium Security Cybersecurity potato stamp of excellence Recommendation Should be your goal Crypto as a ServiceSo now the question is: can we do any better? Secrets are no longer hardcoded, they’re not in the environment, they’re not in the files—they’re in the vault, and authentication to the vault is based on cloud identity. Pretty good, right?Well—yes and no.Let’s get a bit paranoid. What if the application is compromised, and an attacker can execute commands within the application context (e.g., remote code execution)? While there are no secrets in the environment, the running application has the secrets loaded in memory. If the attacker can dump the memory, they can retrieve the secrets—and this is not a particularly difficult thing to do.So, what can we do? Actually, we can raise the security bar a bit higher. Imagine you have a secret used for encrypting data before it’s saved to the database. You need the secret to run the encryption algorithm, but the algorithm doesn’t necessarily need to run in the application itself.Just as we delegated secret storage and access control to the vault, we can also delegate cryptographic operations. There’s even a term for this: Encryption-as-a-Service. The same approach applies to digital signatures and other cryptographic operations.So in this model, the application does not have to have the secret to run the cryptographic operation.Again, the application can be authenticated to the service using cloud identity.This way even if attacker is able to dump the memory, he won’t get the secret.But isn’t attacker able to request an encryption or signature operation? - asks the developer.Well as long as the attacker has the access to environment, he is.But this is much better than attacker stealing the secret. Again we do not eliminate the risk, but we reduce it to acceptable level.In this model we can introduce additional controls, e.g., rate limiting, behavior analysis, etc.Ok, but how to do it? Our app uses a local encryption SDK. - says the developer.Well, this is a good question. Of course cloud solutions have an SDK for that but the challenge is how to integrate itwith the exising code base. Typically cryptographic operations are either provided as native language functionality or libraries.Sometimes it might be a significant development effort to replace the existing code with the external cryptographic service.ExamplesThis example shows a simple encryption API from Azure Key Vault. An application can call this API to encrypt the data.In response, the application gets the encrypted data, but the secret is never exposed to the application.Request:POST https://myvault.vault.azure.net//keys/sdktestkey/f6bc1f3d37c14b2bb1a2ebb4b24e9535/encrypt?api-version=7.4{ \"alg\": \"RSA1_5\", \"value\": \"5ka5IVsnGrzufA\"}Response:{ \"kid\": \"https://myvault.vault.azure.net/keys/sdktestkey/f6bc1f3d37c14b2bb1a2ebb4b24e9535\", \"value\": \"CR0Hk0z72oOit5TxObqRpo-WFGZkb5BeN1C0xJFKHxzdDCESYPCNB-OkiWVAnMcSyu6g2aC8riVRRxY5MC2CWKj-CJ_SMke5X2kTi5yi4hJ5vuOLzmg_M6Bmqib7LsI-TeJHr9rs3-tZaSCdZ2zICeFWYduWV5rPjTnAD98epTorT8AA1zMaYHMIhKpmttcj18-dHr0E0T55dgRtsjK04uC3FlRd3odl4RhO1UHAmYpDd5FUqN-20R0dK0Zk8F8sOtThLhEmuLvqPHOCUBiGUhHA4nRDq1La4SUbThu2KMQJL6BbxxEymuliaYcNNtW7MxgVOf6V3mFxVNRY622i9g\"}Source: Azure Key VaultEvaluation     Effort Significant Security Security nirvana Recommendation Do it if you can SummarySecrets are an unavoidable part of application development, but the way we handle them makesall the difference. Do you feel comfortable with a teammate who hardcodes API keys and waves it off with, “It’s fine; it’s only for testing!”? Or are you ready to explore better options for managing secrets securely? The right solution could be as close as the next library or tool, ready to transform how your team protects sensitive information.By choosing a secure approach from the start (as the “shift left” principle recommends), you can save yourself the hassle of reworking your code later. A little effort upfront goes a long way in keeping your application safe and future-proof.I hope this article gave you some useful ideas on how to handle secrets securely and save yourself from future headaches (and embarrassing leaks)." }, { "title": "CISSP thoughts & notes", "url": "/posts/cissp-notes/", "categories": "Study", "tags": "CISSP", "date": "2024-10-10 04:00:00 +0000", "snippet": "IntroThe Certified Information Systems Security Professional (CISSP) certification is a well-known credential for info security professionals. For me, getting this certification was a long-term goa...", "content": "IntroThe Certified Information Systems Security Professional (CISSP) certification is a well-known credential for info security professionals. For me, getting this certification was a long-term goal. I wanted to formalize my skills and gain some recognition in the cybersecurity world – what better way to prove myself was there?I originally planned to study for CISSP before starting my PhD, hoping to finish it while I was studying. But balancing both was tough, even impossible at times. After I graduated, I finally had time to regain my focus and double down on the certification prep.I scheduled regular study sessions for about three months, putting in 2-3 hours during the week and 4-5 hours on weekends. On September 9, 2024, I passed the CISSP exam after answering just 100 questions!Is it worth it?Before committing to any certification, I always ask if it’s really worth the time and money. Will it truly help my career, or is it just another title to add to the wall? To make a good choice, I talked to friends and colleagues and researched a lot of industry articles and forums. Opinions were mixed, but most agreed the certification can boost career prospects, even if it doesn’t necessarily make you a better cybersecurity professional. I agree with that, but I also keep in mind that the CISSP exam isn’t meant to test hands-on skills. It assumes candidates are already experienced cybersecurity professionals, and (ISC)² verifies this after passing the exam. Having said all that, here are my top 4 reasons for pursuing the CISSP certification.Non-technical aspects of cybersecurityThe CISSP exam is particularly interesting because it bridges the gap between the technical and non-technical aspects of cybersecurity. With a complex background as a software developer and researcher, and having delved deeply into the technical workings of security mechanisms, I realized I had been missing key defense layers that are essential to securing organizations, but are not purely technical.Topics such as data ownership, policies, procedures, risk management, business continuity and legal considerations were relatively new to me. Although I had an intuitive understanding of their importance while working in cybersecurity environments, they were always handled by other stakeholders. Studying for CISSP helped me see how these elements fit into the broader security picture, giving me a more comprehensive understanding of how they all interconnect.Systematization of knowledgeWhen studying for the CISSP exam, you’ll notice that it covers a wide range of topics. The official study guide alone is over 1,000 pages, so there’s a lot to take in. But the exam isn’t about memorizing every protocol or system—it’s more about understanding key concepts, how they’re applied in the real world, and how different aspects of security work together to achieve the best security posture.What I found intriguing during my preparation was learning the backstory behind some technologies—why certain choices were made and how things evolved over time. While that kind of historical insight might not be something you use day-to-day, it’s definitely valuable knowledge.CISSP gives you a big-picture view of cybersecurity, and that was definitely the most useful part for me. As someone with a Linux background, I also appreciated getting to learn about other proprietary security solutions. Overall, the study material offers a really solid overview of cybersecurity, which makes the whole certification process worth it.LanguageIn the industry, every team has its own way of communicating. Managers focus on deliverables and risks, engineers talk about features and deadlines, while security teams discuss vulnerabilities and threats. Studying for the CISSP exam helps you gain a deep understanding of how each group perceives and talks about security.Since communication is a crucial aspect of cybersecurity, CISSP equips you with the language needed to effectively engage with stakeholders, managers, engineers and security professionals alike. The next step is putting this knowledge into practice in real-world situations.Formal RequirementThe CISSP exam primarily validates that you’ve got a solid grasp of non-technical security knowledge, but adding it to your CV can still give you a boost during your next technical job search. Some job listings mention CISSP as a “nice-to-have,” which can help youget through that first HR screen.Final thought: I think certifications like CISSP are best treated as a bonus rather than the main objective. Having a strong GitHub presence, a blog or a good portfolio of projects often goes further in showcasing what you can actually do.Study materialsThere are a ton of books and question banks out there for your CISSP prep–just make sure you’re using most recent ones. I’m a fan of digital materials, but I also enjoy writing notes by hand. I mainly used the Sybex Study Guide and the All-in-One Exam Guide, focusing on the parts I needed. Then I went through hundreds (if not thousands!) of practice questions to find my weak spots and went back to review those areas. Below are the resources that helped me the most:Oreilly libraryIf you can learn from digital materials instead of paper books, Oreilly library is definitely worth considering. It costs about $50 a month (as of October 2024) and gives you access to countless books and videos, including the Sybex Official Study Guide and some exam practice books. As this option is more cost-effective and environmentally friendly than buying physical books, it’s my default preference.Link: https://www.oreilly.com/search/?q=cissp&amp;rows=100Learnzapp Learnzapp is a great resource for practicing questions once you’ve read the books and tackled the questions they include. It offers both a mobile app and a web version, and you can even practice in a simulated exam mode to get a taste of the real attempt.I found it really helpful to see just how mentally draining it can be to answer over 100 questions in one sitting.Link: https://isc2-learnzapp.web.app/homeStudy Notes and Theory Another solid resource is the study materials and questions from Luke Ahmed. His question set is intentionally quite challenging, so I recommend using it after you’ve gone through all the other materials. Just avoid doing these questions right before the exam. Since they can be harder and trickier than the actual test, you might end up a bit discouraged or even completely lose confidence if you can’t hit that passing score!Link: https://www.studynotesandtheory.com/Destination Certification Videos Rob Witcher’s CISSP MindMaps are great for reviewing and pinpointing the areas you need to focus on more. The videos cover all the domains and summarize the key topics effectively.Link:https://www.youtube.com/watch?v=hf5NwUSEkwA&amp;list=PLZKdGEfEyJhLd-pJhAD7dNbJyUgpqI4puTechnical Institute of America I found Andrew Ramdayal’s videos just before the exam, and they really helped me feel confident about the topics. After tackling his 50 CISSP Practice Questions, I knew I was ready to take the exam.Link: https://www.youtube.com/watch?v=qbVY0Cg8NtwSummaryPassing the CISSP exam was a fantastic journey. I learned a ton, and I’m really glad I decided to give it a go. Even before the exam, I wasn’t sure if I was ready, but I tend to second-guess myself (and after all–who is ever fully ready?). I totally recommend CISSP for anyone who’s been in cybersecurity for a few years and wants a good overview of the field. The certification gained is just the cherry on top.Next on my list - looking at you, OSCP!" }, { "title": "OPA for User Registration", "url": "/posts/opa-registration/", "categories": "Authorization, IAM", "tags": "OPA, Intermediate", "date": "2024-06-30 04:00:00 +0000", "snippet": "BackgroundIdentity management is often far more complex in practice than it seems in theory. Simple user journeys quickly become convoluted when edge cases and exceptions are taken into account. Th...", "content": "BackgroundIdentity management is often far more complex in practice than it seems in theory. Simple user journeys quickly become convoluted when edge cases and exceptions are taken into account. This complexity is further amplified when identity management is being implemented for the first time in an organization, where a lack of standardization, identity silos, and poor data quality often present significant obstacles.In many cases, organizations have multiple identity stores tied to different applications, making it difficult to establish a unified view of the user. The process of generating a “golden record” — a single, unique, and well-defined user record compiled from various fragmented sources — becomes a daunting technical challenge. This complexity is further increased when business rules are not clearly defined upfront and need to be discovered and implemented during the process. Additionally, data quality is frequently compromised due to legacy systems and the natural erosion of information over time.Given these challenges, it is crucial to implement a robust and flexible identity management system capable of accommodating complex business rules. The system should use an approach that is both technically sound and easy to understand, minimizing issues during the translation of business logic into code. Furthermore, changes to these business rules should not necessitate a full software development lifecycle, but rather be managed as configuration changes, enabling faster and more adaptable updates.In this article, I explore a solution implementing a centralized policy decision point based on open-source components for just-in-time user registration. Please note that this is not intended as an evidence-based evaluation but rather a description of what worked for me in solving this problem, and it may help you as well. The approach outlined below uses Open Policy Agent (OPA) as a policy engine and has been successfully deployed in production for some time.GoalsFirst, let’s enumerate the goals for the system:FundamentalB1. The solution needs to be production ready and verified in a real-world scenario.B2. We don’t have unlimited budget, so the solution should be cost-effective, easy to maintain and modify.FunctionalF1. Encapsulate business logic in a separate component with easy interface.F2. Enable complex logic on user attributes and relations between them.F3. Fast JIT decision what actions identity management system should take (e.g., deny the process).F4. Generate data modification instructions for the identity management system.F5. Standardised way of defining rules, which can be extended with other use cases if needed.UsabilityU1. Business (aka non-technical employees) can understand the logic of the policy and even define their own rules.U2. Developers can understand the process and do not have to translate business requirements into code.U3. It is easy to introduce changes to the rules and integrate with the existing IDM stack.U4. Versioning following GitOps principles.DesignThe high-level design of the solution is as follows: IDM aggregates data from various sources. IDM creates a data bundle that represents the user. Bundle is sent to the evaluation engine. Evaluation engine uses the rules to make a decision. Evaluation engine returns the decision and actions to IDM.The diagram below shows the high-level design of the solution: flowchart LR A[Data Source A] &amp; B[Data Source B] &amp; C[Data Source C] -- User Data --&gt; IDM IDM -- Aggregated Data --&gt; EV[Evaluation engine] EV[Evaluation engine] -- Result --&gt; IDMProposed solutionEquipped with previous experience with OPA (Open Policy Agent) for access management,I want to use it for the user registration process.OPA allows you to evaluate policies which encode business logic using the rego language.OPA executes declarative policies completely decoupled from the application code. Any party in your system cansimply trigger a REST call to OPA engine with input data for evaluation.For example, below is a sample of a simple policy that checks access policy for API calls to the HTTP server (source OPA documentation):package httpapi.authzimport rego.v1# Bob is Alice's manager, and Betty is Charlie's.subordinates := {\"alice\": [], \"charlie\": [], \"bob\": [\"alice\"], \"betty\": [\"charlie\"]}default allow := false# Allow users to get their own salaries.allow if {\tinput.method == \"GET\"\tinput.path == [\"finance\", \"salary\", input.user]}# Allow managers to get their subordinates' salaries.allow if {\tsome username\tinput.method == \"GET\"\tinput.path = [\"finance\", \"salary\", username]\tsubordinates[input.user][_] == username}In this example based on the request metadata, the OPA engine makes decision to allow or deny access to the resource.For example, the data as shown below sent for the evaluation will result with result true.{ \"method\": \"GET\", \"path\": [\"finance\", \"salary\", \"alice\"], \"user\": \"alice\"}This can be easily extended to the user registration process to decide if a user should be registered or not. What about actions that should be taken by the IDM system? OPA can help with that as well.Instead of returning just true or false, the engine can return a complex JSON object that is constructed accordinglyto the rules shown in the implementation section.AlternativesBefore selecting OPA, I evaluated the approaches of building the rules engine from scratch as a native extension of the identity management system, as well as using a BPMN engine such as Camunda Zeebe.While native implementation is tempting with its flexibility, performance and tight integration with the IDM system, it fails to deliver on the usability side.Source code typically mixes business logic with language specific constructs, making it hard to understand for non-technical (and sometimes technical) employees.On top of that, designing and implementation of the rules engine is a significant development effort increasing the cost of the solution.My first thought of a viable solution that binds business logic and technical implementation was to use the BPMN engine.BRPN (Business Process Model and Notation) defines an XML-based representation of the business flow and binds it with software executions.Importantly, the flow design can be defined in the graphical editor without knowingthe underlying technology. While this approach might have worked, I found it too high-level for the task at hand.In my use case the logic needs to evaluate every single field of the user and make the decision.It would mean that steps of the BPMN had to be very granular which decreses performance (each one calls method in the BPMN worker) or the logic would have to be implemented in the worker which is very close to the native implementation.OPA to the rescueLet’s start with the evaluation of the requirements.OPA is definitely production-ready and tested by many companies (B1).The open-source version is sufficient for this use case, and the running cost is relatively small.The policy is defined as a text file and can be modified in any editor (B2). This completes our fundamental requirements.Regarding functional requirements, Rego files contain the entire business logic.The Rego file is evaluated in the OPA engine, a separate component (e.g., a Docker container) with a clear REST API.To get a policy decision, literally one REST API call is needed, so we have F1 covered.In the case of F2, the Rego language is powerful; however, I must admit that, with a typical software development mindset (i.e., writing step-by-step instructions to achieve results), defining the rules might not feel intuitive. For example, there are no for loops in Rego, and if statements are declared differently than what you might be familiar with in popular languages like Java or JavaScript (see more in rego design principles).However, thanks to this design, complex policies with broad and deep execution paths can be described concisely.Conditions between static data, input data, and dynamically calculated data can be easily defined.One drawback from my experience with Rego is that it is not a full-fledged programming language, and some operations available by default or through native libraries are missing. For example, simple geofencing calculations require basic mathematical operations like sine and cosine, which are not available in Rego. However, you can extend the Rego engine by writing Go code and including it in the OPA engine. Alternatively, in my case, I queried an external service for the result using OPA’s HTTP client.Therefore, I believe that the requirement F2 is sufficiently met.F3 is a strong point of OPA. The engine is very fast and can evaluate policies in a fraction of a second.For our complex policy (with over 2,000 lines of code), the evaluation time is between 2-4ms.Though this is not a benchmark result suitable for OPA performance comparisons, the outcome is far better than required for our use case, so F3 is met.OPA policies are not limited to returning a single boolean value. They can return a complex JSON object.With that, you can use policy rules to construct a JSON object that orchestrates actions to be taken by the IDM system.Actions like modifying a user’s email in Active Directory or updating contact information in the CRM system can be easily defined.In my case, OPA rules were used to construct LDAP LDIF files that were later executed by the IDM system.Therefore, F4 is met.Making complex rules simple is a challenge, and regardless of the technology, one can end up with spaghetti code.However, Rego has the advantage of flattening the logic and promoting the creation of short blocks that represent policy conditions.This contributes to the readability and standardization of rule definitions. Regarding extensibility, once created, a policy can be easily extended—up to the point that it breaks the agreed interface with the client (e.g., a new field in the response).But even then, the change is easy to introduce and does not require much effort.Therefore, F5 is met.Now let’s look at usability.To my surprise, OPA was easy to understand for the business. Once rules are named in understandable language (e.g., is_user_in_hr_group), the rule and entire policy become more like a recipe than code.I found that after initial training, non-technical employees were able to understand the policy and even define their own rules.Therefore, in my opinion, U1 is met.Easily understandable but technical representations of policy rules also benefit the development side.Business requirements, typically delivered as a set of plain English-described conditions, are much easier to translate into Rego code than into native code.The reverse is also true: once requirements are expressed in Rego, it is easier to perform reverse reasoning and understand the business logic.Therefore, U2 is met.Regarding U3, I have already discussed policy updates and integration via a single REST call.In terms of versioning (U4), rego files are text files that can be easily versioned using Git.Implementation examplesBelow, I share a few samples of how the policy can be implemented. PLease note that those exmaples are notcomplete and are simplified for the purpose of this article.Ex1. The policy is designed to have a single entry point. Below, you can see a rule called decision, which has a similarfunction as the main method in many programming languages. It is executed directly from the REST API, and defines the structure of the response. It aggregates results from other rules into a single JSON body.decision := response { response := { \"ruleImplemented\":rule_implemented, \"input\": policy_input, \"ldap\": ldap, \"error\": error, \"warn\": warn, \"metadata\": metadata, \"version\": policy_version, \"timestamp\": time.format(time.now_ns()) }}Ex2. The policy also runs the check of the data quality. If something is wrong thewarning message is added to the response. A similar approach is done for errors.warn[warn] { user_is_in_ldap count(ldap_attribute_x) == 0 warn := \"MISSING_LDAP_ATTRIBUTE_X: LDAP data is missing the x attribute\"}Ex3. As I mentioned before, the policy also creates steps for other systems to execute. The example below shows how an ldif modifyinstruction is created for an LDAP system.ldap := ldap_recs {\tldap_recs := {\t\t\"roles\":[{\t\t\t\"dn\": \"cn=member,ou=roles,dc=example,dc=com\",\t\t\t\"changetype\": \"modify\",\t\t\t\"add\": \"member\",\t\t\t\"member\": concat(\",\",[concat(\"=\",[\"uid\",input.username]),\"ou=users,ou=external,dc=example,dc=com\"])\t\t}]\t}}SummaryIn this article, I demonstrated how Open Policy Agent (OPA) can be used to implement centralized policy evaluation for just-in-time user registration. The solution is production-ready, cost-effective, and easy to maintain and modify. Additionally, it has the potential to be extended to other use cases, which I am currently exploring.The key takeaways are that, despite an initial learning curve, OPA and its Rego language offer powerful capabilities for implementing complex business logic. Another important point is that OPA can return more than just policy decisions, offering flexibility in its responses. Finally, I believe that the simplicity of Rego’s syntax, combined with well-structured naming conventions, can significantly reduce the gap between business requirements and technical implementation, minimizing friction and misunderstandings." } ]
